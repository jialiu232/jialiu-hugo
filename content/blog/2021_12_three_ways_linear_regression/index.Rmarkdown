---
title: "3 functions to perform linear regression in Python"
author: "Jia Liu"
date: '2021-12-12'
excerpt: Differentiate among three python functions for linear regression
subtitle: ''
draft: no
images: null
series: null
tags: null
categories: null
layout: single
#output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Sys.setenv(RETICULATE_PYTHON = "/Users/liujia/opt/anaconda3/bin/python")
library(reticulate)
#use_python("/Users/liujia/opt/anaconda3/bin/python", require = TRUE)
Sys.which("python")
```


```{r, include=FALSE}
# Make sure python3 in installed
#path_2_python3 <- system("which python3")
#if (path_2_python3 == "") message("Python3 not found.")


# See: https://cran.r-project.org/web/packages/reticulate/vignettes/python_packages.html
reticulate::py_config() 
Sys.which("python")
#py_install(c("pandas", "numpy", "matplotlib"))
# Check the available conda environments
#conda_list()

# indicate that we want to use a specific condaenv
#use_condaenv("r-reticulate")

# install pandas
#py_install("scikit-learn")
#conda_install("r-reticulate", "pandas")
#conda_install("r-reticulate", "matplotlib")
#conda_install("r-reticulate", "statsmodels")
#conda_install("r-reticulate", "seaborn")

# Check if libraries are imported successfully
#if (reticulate::py_module_available("pandas")) message("'pandas' found.")
#if (reticulate::py_module_available("numpy")) message("'numpy' found.")
#if (reticulate::py_module_available("matplotlib")) message("'matplotlib' found.")
```

## So why are we here?

Recently I helped another PhD student in our department to solve a problem in her python script that performs linear regression. In this script, two linear regression models based on the input data were generated using two methods (`LinearRegression()` from `sklearn.linear_model`; the `OLS` function from `statsmodels.api`). The [R-square](https://www.investopedia.com/terms/r/r-squared.asp) as well as [Adjusted R-Square](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/) calculated from these two models were very different. That inspired me to start this post: while there are always different methods that performs the same process, disagreement among the results generated from different methods can be a headache. Here we will go through three widely used methods that build linear regression models in python: `sklearn`, `statsmodel.api`, and `statsmodels.formula.api` using a randomly generated dataset, and see how we can avoid the problem. 

---

## Generate a Random dataset

We will first import packages that needed, and then generate a random dataset:

```{python}
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

import statsmodels.api as sm


# Generate 'random' data
np.random.seed(0)
X = 2.5 * np.random.randn(150) + 1 # Array of 150 values with mean = 1, stddev = 2.5
res = 0.5 * np.random.randn(150) # Generate 150 residual/random error terms
y = 4 + 0.3 * X + res # Get the actual values of Y assuming y_i = 4 + 0.3x_i + e_i 
 
# Create pandas dataframe to store our X and y values
df = pd.DataFrame(
    {'X': X,
     'y': y}
)

df.head()
```


Let's visualize the regression:

```{python}
sns.set_theme(color_codes=True)
sns.regplot(x="X", y="y", data = df).set(title = "y vs X")
```

There seems to be a positive trend between $y$ and $X$, but there's also a certain extent of dispersion of data points from the predicted regression line as shown in the figure. 


## Linear regression

Since our goal here is to show how to apply linear regression using three methods in python, but not analyze any data, we will simply build the models based on the simulated data, make predictions using the predictor $X$ in the simulated data, and get the summaries. 
 

### Use Sklearn

Build and fit the model:

```{python}
from sklearn.linear_model import LinearRegression

model_sk = LinearRegression()
model_sk.fit(df[['X']], df['y'])

print(f'The coefficient of `model_sk` is {model_sk.coef_[0]}')
print(f'The intercept of `model_sk` is {model_sk.intercept_}')
```

Get the R square: 

```{python}
# Calculate R square
R2 = model_sk.score(df[['X']], df['y']) 
print(f'The R square of `model_sk` is {R2}')
```

There seems to be no built-in function for adjusted R square in `LinearRegression` of `sklearn`. As we know $$R_{adj}^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}$$  where $n$: the number of subjects in the sample; $k$: the number of variables, we can manually calculate the adjusted R square: 

```{python}
# Calculate adjusted R square
#R2 = model_sk.score(df[['X']], df['y'])
n = len(X)
p = 1

adj_R2 = 1-(1-R2)*(n-1)/(n-p-1)
print(f'The adjusted R square of `model_sk` is {adj_R2}')
```


### Use statsmodel.api

`statsmodels.api.OLS` can be used for ordinary least squares model. One thing to notice as mentioned in the [Statsmodels API](https://tedboy.github.io/statsmodels_doc/generated/generated/statsmodels.api.OLS.html) is: *...An intercept is not included by default and should be added by the user. See statsmodels.tools.add_constant()...*. Including intercept or not can make a drastic difference on the model. Let's have a look.

- Default: Without intercept

Model summary: 

```{python}
import statsmodels.api as sm
model_statsm = sm.OLS(y, X)
results_1 = model_statsm.fit()
print(results_1.summary())
```

Parameters: 

```{python}
results_1.params
```




- Add an intercept

Model summary: 

```{python}
import statsmodels.api as sm
X_cons = sm.add_constant(X)
model_statsm = sm.OLS(y, X_cons)
results_2 = model_statsm.fit()
print(results_2.summary())
```

Parameters: 

```{python}
results_2.params
```

* **Summary:**

    + In our simulated data, we assumed there is an intercept. Thus the default model of `statsmodel.api` that assumes no intercept didn't fit the data very well. 

    + Coefficients generated from the model with intercept are similar to the ones used to simulate the data; R-square and adjusted R square are close to the ones from `sklearn` model 

    + We can include an intercept to a `statsmodel.api` model by using `sm.add_constant()` function to add a constant variable to $X$ as in the example above. 



### Use Statsmodels formula

`statsmodels.formula.api` allows users to define the regression model using a R-like formula. 

- Default: With intercept

Note that unlike `statsmodel.api`, the default ols model made by `statsmodels.formula.api` includes an intercept:

```{python}
from statsmodels.formula.api import ols

formula = 'y ~ X'
model_sfa = ols(formula = formula, data = df).fit()
model_sfa.summary()
```

Parameters predicted from the above model are close to the coefficients we used to simulate the data. The R square and adjusted R-square also agree with the previous `sklearn` and `statsmodel.api` (with intercept) methods. Great! 


- Exclude the intercept

To build a model without an intercept using `statsmodels.formula.api` (data may already been mean-centered), we can define the formula as $formula = \text{'}y \sim  X - 1\text{'}$ (`-1` will exclude the intercept):

```{python}
from statsmodels.formula.api import ols

formula = 'y ~ X-1'
model_sfa = ols(formula = formula, data = df).fit()
model_sfa.summary()
```



